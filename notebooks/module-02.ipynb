{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "source": [
    "# Module 02: Plan and implement Azure Cosmos DB SQL API\n",
    "\n",
    "- [[Learning path]](https://docs.microsoft.com/en-us/learn/paths/plan-implement-azure-cosmos-db-sql-api/?ns-enrollment-type=Collection&ns-enrollment-id=1k8wcz8zooj2nx)\n",
    "- [[Lab]](https://microsoftlearning.github.io/dp-420-cosmos-db-dev/instructions/02-configure-throughput.html): Configure throughput for Azure Cosmos DB SQL API with the Azure portal\n",
    "- [[Lab]](https://microsoftlearning.github.io/dp-420-cosmos-db-dev/instructions/03-migrate-data.html): Migrate existing data using Azure Data Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan Resource Requirements\n",
    "## Understand throughput\n",
    "### Container-level throughput provisioning\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/learn/wwl-data-ai/plan-resource-requirements/media/2-container.png)\n",
    "\n",
    "### Database-level throughput provisioning\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/learn/wwl-data-ai/plan-resource-requirements/media/2-database.png)\n",
    "\n",
    "### Mixed-throughput provisioning\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/learn/wwl-data-ai/plan-resource-requirements/media/2-mixed.png)\n",
    "\n",
    "## Evaluate throughput requirements\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/learn/wwl-data-ai/plan-resource-requirements/media/3-request-units.png)\n",
    "\n",
    "| **Operation type** | **Number of requests per second** | **Number of RU per request** | **RU/s needed** |\n",
    "| ---: | :---: | :---: | :--- |\n",
    "| **Write Single Document** | 10,000 | 10 | 100,000 |\n",
    "| **Top Query #1** | 700 | 100 | 70,000 |\n",
    "| **Top Query #2** | 200 | 100 | 20,000 |\n",
    "| **Top Query #3** | 100 | 100 | 10,000 |\n",
    "| **Total RU/s** | | | 200,000 RU/s |\n",
    "\n",
    "## Evaluate data storage requirements\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/learn/wwl-data-ai/plan-resource-requirements/media/4-calculator.png)\n",
    "\n",
    "## Time-to-live (TTL)\n",
    "\n",
    "The TTL value for a container is configured using the ``DefaultTimeToLive`` property of the container's JSON object.\n",
    "\n",
    "| **DefaultTimeToLive** | **Expiration** |\n",
    "| --- | --- |\n",
    "| *Does not exist* | Items are not automatically expired |\n",
    "| ``-1`` | Items will not expire by default |\n",
    "| *n* | *n* seconds after last modified time |\n",
    "\n",
    "Example:\n",
    "\n",
    "| **Container.DefaultTimeToLive** | **Item.ttl** | **Expiration in seconds** |\n",
    "| :--- | :--- | ---: |\n",
    "| ``1000`` | *null* | ``1000`` |\n",
    "| ``1000`` | ``-1`` |  *This item will never expire* |\n",
    "| ``1000`` | ``2000`` | ``2000`` |\n",
    "\n",
    "Another example: \n",
    "\n",
    "| **Container.DefaultTimeToLive** | **Item.ttl** | **Expiration in seconds** |\n",
    "| :--- | :--- | ---: |\n",
    "| *null* | *null* | *This item will never expire* |\n",
    "| *null* | ``-1`` |  *This item will never expire* |\n",
    "| *null*| ``2000`` | *This item will never expire* |\n",
    "\n",
    "## Plan for data retention with time-to-live (TTL)\n",
    "\n",
    "Consider solutions such to aggregate and migrate data such as:\n",
    "- Change feed\n",
    "- Azure Data Warehouse\n",
    "- Azure Blob Storage\n",
    "\n",
    "# Configure Azure Cosmos DB SQL API throughput\n",
    "\n",
    "## Serverless\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/learn/wwl-data-ai/configure-azure-cosmos-db-sql-api/media/2-serverless.png)\n",
    "\n",
    "## Compare serverless vs. provisioned throughput\n",
    "\n",
    "| **Throughput**  | **Workloads** | **RUs** | **Global Distribution** | **Compare storage limits** |\n",
    "|---|---|---|---|---|\n",
    "| **Provisioned** | Ideal for workloads with predictable traffic patterns | Number RUs per second preset to each container | Can distribute data to an unlimited number of Azure regions | Unlimited data in a container |\n",
    "| **Serverless**  | Can handle workloads that have wildly varying traffic | Doesn't require any planning or automatic provisioning | Accounts can only run in a single Azure region | Up to 50 GB of data in a container |\n",
    "\n",
    "## Autoscale\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/learn/wwl-data-ai/configure-azure-cosmos-db-sql-api/media/4-autoscale-2.png)\n",
    "\n",
    "## Compare autoscale vs. standard (manual) throughput\n",
    "\n",
    "| **Scaling**  | **Workloads** | **RUs** | **Scenarios** | **Rate-limiting** |\n",
    "|---|---|---|---|---|\n",
    "| **Standard** | Suited for workloads with steady traffic | Requires a static number of request units to be assigned ahead of time | Where the application throughput can be accurately predicted | Since the RU/s are static, requests beyond this will be rate-limited |\n",
    "| **Autoscale**  | Suited for unpredictable traffic | You only set the maximum RUs | Where the application throughput can't be accurately predicted, but an acceptable max throughput can be assigned | Will scale up to the max RU/s before similarly rate-limiting responses |\n",
    "\n",
    "\n",
    "# Moving data into and out of Azure Cosmos DB SQL API\n",
    "\n",
    "## Move data by using Azure Data Factory\n",
    "\n",
    "### Linked service\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"<example-name-of-linked-service>\",\n",
    "    \"properties\": {\n",
    "        \"type\": \"CosmosDb\",\n",
    "        \"typeProperties\": {\n",
    "            \"connectionString\": \"AccountEndpoint=<cosmos-endpoint>;AccountKey=<cosmos-key>;Database=<cosmos-database>\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/azure/data-factory/media/connector-azure-cosmos-db/azure-cosmos-db-connector.png)\n",
    "![image]()https://docs.microsoft.com/en-us/azure/data-factory/media/connector-azure-cosmos-db/configure-azure-cosmos-db-linked-service.png\n",
    "\n",
    "### Read from Azure Cosmos DB\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"source\": {\n",
    "        \"type\": \"CosmosDbSqlApiSource\",\n",
    "        \"query\": \"SELECT id, categoryId, price, quantity, name FROM products WHERE price > 500\",\n",
    "        \"preferredRegions\": [\n",
    "            \"East US\",\n",
    "            \"West US\"\n",
    "        ]        \n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Write to Azure Cosmos DB\n",
    "\n",
    "```json\n",
    "\"sink\": {\n",
    "    \"type\": \"CosmosDbSqlApiSink\",\n",
    "    \"writeBehavior\": \"upsert\"\n",
    "}\n",
    "```\n",
    "\n",
    "## Move data by using a Kafka connector\n",
    "\n",
    "### Write to Azure Cosmos DB\n",
    "\n",
    "```bash\n",
    "# Create a new topic named prodlistener\n",
    "kafka-topics --create \\\n",
    "    --zookeeper localhost:2181 \\\n",
    "    --topic prodlistener \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 1\n",
    "\n",
    "# Start producer\n",
    "kafka-console-producer \\\n",
    "    --broker-list localhost:9092 \\\n",
    "    --topic hotels\n",
    "```\n",
    "\n",
    "### Read from Azure Cosmos DB\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"cosmosdb-source-connector\",\n",
    "  \"config\": {\n",
    "    \"connector.class\": \"com.azure.cosmos.kafka.connect.source.CosmosDBSourceConnector\",\n",
    "    \"tasks.max\": \"1\",\n",
    "    \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "    \"connect.cosmos.task.poll.interval\": \"100\",\n",
    "    \"connect.cosmos.connection.endpoint\": \"<cosmos-endpoint>\",\n",
    "    \"connect.cosmos.master.key\": \"<cosmos-key>\",\n",
    "    \"connect.cosmos.databasename\": \"<cosmos-database>\",\n",
    "    \"connect.cosmos.containers.topicmap\": \"<kafka-topic>#<cosmos-container>\",\n",
    "    \"connect.cosmos.offset.useLatest\": false,\n",
    "    \"value.converter.schemas.enable\": \"false\",\n",
    "    \"key.converter.schemas.enable\": \"false\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Move data by using Stream Analytics\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/data/images/stream-processing-asa/stream-processing-asa.png)\n",
    "\n",
    "| **Property** | **Description** |\n",
    "| ---: | :--- |\n",
    "| ``Output alias`` | An alias to refer to this output in the query |\n",
    "| ``Account ID`` | Account endpoint URI |\n",
    "| ``Account Key`` | Account key |\n",
    "| ``Database`` | Name of the database resource |\n",
    "| ``Container name`` | Name of the container |\n",
    "\n",
    "## Move data by using the Azure Cosmos DB Spark connector\n",
    "\n",
    "![image](https://docs.microsoft.com/en-us/azure/cosmos-db/media/synapse-link/synapse-analytics-cosmos-db-architecture.png)\n",
    "\n",
    "```bash\n",
    "# Setup the cosmos db with synapse link enabled (--enable-analytical-storage true and --analytical-storage-ttl -1)\n",
    "az cosmosdb create --name <name> --resource-group <resource-group> --enable-analytical-storage true\n",
    "az cosmosdb sql container create --resource-group <resource-group> --account <account> --database <database> --name <name> --partition-key-path <partition-key-path> --throughput <throughput> --analytical-storage-ttl -1\n",
    "```\n",
    "\n",
    "### Read from Azure Cosmos DB\n",
    "\n",
    "```python\n",
    "# Read from synapse link\n",
    "productsDataFrame = spark.read.format(\"cosmos.olap\")\\\n",
    "    .option(\"spark.synapse.linkedService\", \"cosmicworks_serv\")\\\n",
    "    .option(\"spark.cosmos.container\", \"products\")\\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### Write to Azure Cosmos DB\n",
    "\n",
    "\n",
    "```python\n",
    "# Write to OLTP\n",
    "productsDataFrame.write.format(\"cosmos.oltp\")\\\n",
    "    .option(\"spark.synapse.linkedService\", \"cosmicworks_serv\")\\\n",
    "    .option(\"spark.cosmos.container\", \"products\")\\\n",
    "    .mode('append')\\\n",
    "    .save()\n",
    "\n",
    "# Write stream\n",
    "query = productsDataFrame\\\n",
    "    .writeStream\\\n",
    "    .format(\"cosmos.oltp\")\\\n",
    "    .option(\"spark.synapse.linkedService\", \"cosmicworks_serv\")\\\n",
    "    .option(\"spark.cosmos.container\", \"products\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/runIdentifier/\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "C#"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
